<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Muhammad Naufil</title>

    <meta name="author" content="Muhammad Naufil">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
    <style>
      /* Define the styles for the menu bar */
      .menu-bar {
            background-color: #FFFFFF;
            color: #fff;
            overflow: hidden;
        }

        /* Style the individual menu items */
        .menu-item {
            float: right;
            display: block;
            padding: 15px 20px;
            text-align: center;
            text-decoration: none;
        }

        /* Add a hover effect */
        .menu-item:hover {
            background-color: #555;
        }
  </style>

  </head>

  <body>
    <div class="menu-bar">
      <a class="menu-item" href="/talks.html" onclick="loadTalks()">Talks</a>
      <a class="menu-item" href="/">Home</a>
    </div>

    <script>
      // JavaScript function to load talks.html dynamically
      function loadTalks() {
          // Use AJAX or a similar method to load talks.html content into a div on the current page
          // Here's a simple example using fetch:
          fetch('/talks.html')
              .then(response => response.text())
              .then(data => {
                  // Replace the content of a div with the loaded HTML
                  document.getElementById('content-container').innerHTML = data;
              })
              .catch(error => console.error('Error loading talks.html:', error));
      }
  </script>


    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Muhammad Naufil
                </p>
                <p>I am a computer vision research engineer at <a href="https://retrocausal.ai">Retrocausal</a> in Karachi, where I work on human action recognition and 3D human pose estimation.
                </p>
                <p>
                  At Retrocausal I've worked on <a href="https://retrocausal.ai/">AI Copilot for Manufacturing Assembly Optimization</a>,  <a href="https://retrocausal.ai/guidance-analytics-and-trace/">Guidance Analytics and Trace</a> and <a href="https://retrocausal.ai/ergonomic-safety/">Ergonomic Safety</a>.
                </p>
                <p>
                  I'm interested in Vision Language Models, 3D vision, 3D Human Pose Estimation, and Human Action Recognition.
                </p>
                <p style="text-align:center">
                  <a href="mailto:m.naufil1@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=LHvJ9CUAAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/mnauf/">Github</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/mnauf/">Linkedin</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/Muhammad_Naufil.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/photo_circle.png" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Selected Publications</h2>
              </td>
            </tr>
          </tbody></table>

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <!-- <tr onmouseout="zipnerf_stop()" onmouseover="zipnerf_start()"  bgcolor="#FFFFFF">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <img src='images/permutation.jpg' width="160">
          </div>
          <script type="text/javascript">
            function zipnerf_start() {
              document.getElementById('zipnerf_image').style.opacity = "1";
            }

            function zipnerf_stop() {
              document.getElementById('zipnerf_image').style.opacity = "0";
            }
            zipnerf_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://retrocausal.ai/rp-5-permutation-aware-action-segmentation-via-unsupervised-frame-to-segment-alignment/">
            <span class="papertitle">Permutation-Aware Action Segmentation via Unsupervised Frame-to-Segment Alignment</span>
          </a>
          <br>
          <a href="https://scholar.google.com.au/citations?user=8JbK0V8AAAAJ&hl=en">Quoc-Huy Tran</a>,
          <a href="https://pk.linkedin.com/in/ahmedrazamehmood">Ahmed Raza Mehmood</a>,
          <a href="https://pk.linkedin.com/in/muhammad4hmed">Muhammad Ahmed</a>,
          <strong>Muhammad Naufil</strong>,
          <a href="https://pk.linkedin.com/in/anas-zafar/">Anas Zafar</a>,
          <a href="https://www.linkedin.com/in/andreykonin">Andrey Konin</a>,
          <a href="https://scholar.google.com/citations?user=xE_pSDAAAAAJ&hl=en">Zeeshan Zia</a>
          <br>
          Submitted to <em>WACV</em>, 2023 &nbsp
          <br>
          <a href="https://arxiv.org/pdf/2305.19478">arXiv</a> |
          <a href="https://www.youtube.com/watch?v=ZgvbwD3h-fc">Youtube</a>
          <p></p>
          <p>
            This paper presents an unsupervised transformer-based framework for temporal activity segmentation which leverages not only frame-level cues but also segment-level cues. This is in contrast with previous methods which often rely on frame-level information only. Our approach begins with a frame-level prediction module which estimates framewise action classes via a transformer encoder. 
            The frame-level prediction module is trained in an unsupervised manner via
            temporal optimal transport. To exploit segment-level information, we utilize a segment-level prediction module and a
            frame-to-segment alignment module. The former includes
            a transformer decoder for estimating video transcripts,
            while the latter matches frame-level features with segmentlevel features, yielding permutation-aware segmentation results. Moreover, inspired by temporal optimal transport,
            we introduce simple-yet-effective pseudo labels for unsupervised training of the above modules. Our experiments on
            four public datasets, i.e., 50 Salads, YouTube Instructions,
            Breakfast, and Desktop Assembly show that our approach
            achieves comparable or better performance than previous
            methods in unsupervised activity segmentation.          
          </p>
        </td>
      </tr> -->

      <tr onmouseout="zipnerf_stop()" onmouseover="zipnerf_start()"  bgcolor="#FFFFFF">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <!-- <div class="two" id='zipnerf_image'>
              <video  width=100% height=100% muted autoplay loop>
            <source src="images/zipnerf.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div> -->
            <img src='images/action_segmentation.png' width="160">
          </div>
          <script type="text/javascript">
            function zipnerf_start() {
              document.getElementById('zipnerf_image').style.opacity = "1";
            }

            function zipnerf_stop() {
              document.getElementById('zipnerf_image').style.opacity = "0";
            }
            zipnerf_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://retrocausal.ai/action-segmentation-using-2d-skeleton-heatmaps/">
            <span class="papertitle">Action Segmentation Using 2D Skeleton Heatmaps</span>
          </a>
          <br>
          <a href="https://pk.linkedin.com/in/syedwaleedhyder">Syed Waleed Haider</a>,
          <a href="https://pk.linkedin.com/in/muhammad-usama-630722206">Muhammad Usama</a>,
          <a href="https://pk.linkedin.com/in/anas-zafar/">Anas Zafar</a>,
          <strong>Muhammad Naufil</strong>,
          <a href="https://www.linkedin.com/in/andreykonin">Andrey Konin</a>,
          <a href="https://scholar.google.com/citations?user=xE_pSDAAAAAJ&hl=en">Zeeshan Zia</a>,
          <a href="https://scholar.google.com.au/citations?user=8JbK0V8AAAAJ&hl=en">Quoc-Huy Tran</a>,
          <br>
          Submitted to <em>ICRA</em>, 2023 &nbsp
          <br>
          <!-- <a href="http://jonbarron.info/zipnerf">project page</a> -->
          <!-- / -->
          <!-- <a href="https://www.youtube.com/watch?v=xrrhynRzC8k">video</a> -->
          <!-- / -->
          <a href="https://arxiv.org/pdf/2309.06462.pdf">arXiv</a>
          <p></p>
          <p>
            This paper presents a 2D skeleton-based action
            segmentation method with applications in fine-grained human
            activity recognition. In contrast with state-of-the-art methods
            which directly take sequences of 3D skeleton coordinates
            as inputs and apply Graph Convolutional Networks (GCNs)
            for spatiotemporal feature learning, our main idea is to use
            sequences of 2D skeleton heatmaps as inputs and employ
            Temporal Convolutional Networks (TCNs) to extract spatiotemporal features. Despite lacking 3D information, our approach
            yields comparable/superior performances and better robustness
            against missing keypoints than previous methods on action
            segmentation datasets. Moreover, we improve the performances
            further by using both 2D skeleton heatmaps and RGB videos
            as inputs. To our best knowledge, this is the first work to utilize
            2D skeleton heatmap inputs and the first work to explore 2D
            skeleton+RGB fusion for action segmentation.        
          </p>
        </td>
      </tr>


      <tr onmouseout="zipnerf_stop()" onmouseover="zipnerf_start()"  bgcolor="#FFFFFF">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <!-- <div class="two" id='zipnerf_image'><video  width=100% height=100% muted autoplay loop>
            <source src="images/zipnerf.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div> -->
            <img src='images/ismar.jpg' width="160">
          </div>
          <script type="text/javascript">
            function zipnerf_start() {
              document.getElementById('zipnerf_image').style.opacity = "1";
            }

            function zipnerf_stop() {
              document.getElementById('zipnerf_image').style.opacity = "0";
            }
            zipnerf_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://ieeexplore.ieee.org/document/9974204">
            <span class="papertitle">AI-mediated Job Status Tracking in AR as a No-Code service</span>
          </a>
          <br>
          <a href="https://www.linkedin.com/in/andreykonin">Andrey Konin</a>,
          <a href="https://pk.linkedin.com/in/shakeebsiddiqui">Shakeeb Siddiqui</a>,
          <a href="https://pk.linkedin.com/in/hasan-gilani">Hasan Gilani</a>,
          <a href="https://pk.linkedin.com/in/muhammad-mudassir-7024391ab">Muhammad Mudassir</a>,
          <a href="https://pk.linkedin.com/in/hassan11196">Hassan Ahmed</a>,
          <a href="https://www.linkedin.com/in/tabanshaukat/">Taban Shaukat</a>,
          <strong>Muhammad Naufil</strong>,
          <a href="https://scholar.google.com.pk/citations?user=NaF7zqYAAAAJ&hl=en">Awais Ahmed</a>,
          <a href="https://scholar.google.com.au/citations?user=8JbK0V8AAAAJ&hl=en">Quoc-Huy Tran</a>,
          <a href="https://scholar.google.com/citations?user=xE_pSDAAAAAJ&hl=en">Zeeshan Zia</a>
          <br>
          Published in <em>ISMAR</em>, 2022 &nbsp
          <br>
          <a href="https://ieeexplore.ieee.org/document/9974204">IEEE</a>
          <p></p>
          <p>
            Sophisticated AR experiences that can track the status of a manual assembly or maintenance job and provide corresponding guidance are built by teams of computer vision engineers over several months. These engineers model temporal causation on top of object detection and human skeletal tracking provided by traditional AR and AI SDKs. We demonstrate our Pathfinder system which automatically builds computational models of a complex manual task, such as a manufacturing assembly activity, given recorded demonstrations of the task - without requiring any custom coding. These computational models enable ordinary AR developers to build AI-mediated feedback in minutes instead of months. Keywords: Task Guidance, Computer Vision, Human Computer Interaction, Machine Learning, Industrial Applications     
          </p>
        </td>
      </tr>
  </tbody></table>

  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
    <tr>
      <td>
        <h2>Work Experience</h2>
      </td>
    </tr>
  </tbody></table>
      
      
  <table width="100%" align="center" border="0" cellpadding="20"><tbody>
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/assembly_copilot.jpg" alt="clean-usnob" width="160" height="160">
      </td>
      <td width="75%" valign="middle">
        <a href="https://www.youtube.com/watch?v=cLnrC9pn3j8">
          <span class="papertitle">Manual Assembly Copilot</span>
        </a>
        <br>
        <em>Retrocausal</em>
        <br>
        <p>Empower your operators, engineers, and managers to dramatically boost the quality and productivity of your manual processes. Create digital mistake-proofing mechanisms for a variety of assembly and packing processes. Pathfinder tracks individual steps of an assembly process, and offers audible and visual alerts to help associates avoid mistakes.</p>
      </td>
    </tr>
  </tbody></table>

  <table width="100%" align="center" border="0" cellpadding="20"><tbody>
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/analytics.png" alt="clean-usnob" width="160" height="160">
      </td>
      <td width="75%" valign="middle">
        <a href="https://retrocausal.ai/guidance-analytics-and-trace/">
          <span class="papertitle">Guidance, Analytics and Trace</span>
        </a>
        <br>
        <em>Retrocausal</em>
        <br>
        <p>Collects timing data for every cycle performed on a line, at a fine-grained level. It identifies non-value add work as well as most efficiently performed cycles. These capabilities directly aid industrial engineers in improving processes. Additionally, Pathfinder grades individual assembly sessions, versus ideal number of cycles that could have been performed and considering operator mistakes. This further helps engineers compare and contrast different sessions and work styles to rapidly improve processes.</p>
      </td>
    </tr>
  </tbody></table>

  <table width="100%" align="center" border="0" cellpadding="20"><tbody>
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/ergo_sketch.png" alt="clean-usnob" width="160" height="160">
      </td>
      <td width="75%" valign="middle">
        <a href="https://retrocausal.ai/ergonomic-safety/">
          <span class="papertitle">Monocular Camera MoCap System</span>
        </a>
        <br>
        <em>Retrocausal</em>
        <br>
        <p>Pathfinder provides computer vision based ‚Äúin-process‚Äù health and safety analytics as part of the platform. The solution allows analyzing videos recorded from ordinary phone cameras, either using Pathfinder Android or iPhone app or by uploading videos through the web portal. Pathfinder uses its state-of-the-art computer vision technology to compute 3D skeletal poses and extract 3D joint angles. Our technology is optimized for industrial use cases emphasizing a certain degree of robustness to partial obfuscation as well as extreme postures of the human body. This has significant advantages over wearables, goniometers, or Marker-Based Motion Capture Systems. It allows flexible application include the use of a moving camera, and in adapting to crammed or crowded spaces and is therefore easier to scale. Finally the computer vision solution typically captures more information than a small group of markers, e.g. regarding body shape.</p>
      </td>
    </tr>
  </tbody></table>

  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
    <tr>
      <td>
        <h2>Projects</h2>
      </td>
    </tr>
  </tbody></table>
      
      
  <table width="100%" align="center" border="0" cellpadding="20"><tbody>
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="https://user-images.githubusercontent.com/41832069/246027006-5fe0e6e5-1964-49b9-843e-9fcead65da05.png" alt="clean-usnob" width="160" height="160">
      </td>
      <td width="75%" valign="middle">
        <a href="https://www.linkedin.com/posts/mnauf_fyp-selfdrivingcar-tesla-activity-6722547508100272128-STbD?utm_source=share&utm_medium=member_desktop">
          <span class="papertitle">Supervised Learning Based Autonomous Car</span>
        </a>
        <br>
        <strong>Muhammad Naufil</strong>, <a href="https://www.linkedin.com/in/faiz-ur-rehman/">Faiz ur Rehman</a>, <a href="https://pk.linkedin.com/in/syed-haziq-222b94193">Syed Muhammad Haziq</a>, <a href="https://www.linkedin.com/in/danish-khalid-aa8293137/">Danish Khalid</a>
        <br>
        <em>Bachelor Thesis Project</em>, 2020 at NED University of Engineering & Technology
        <p>Predicts steering wheel angle, detects object and classifies traffic signal lights at 4 FPS on a Jetson Nano.</p>
      </td>
    </tr>
  </tbody></table>

  <table width="100%" align="center" border="0" cellpadding="20"><tbody>
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/ships.png" alt="clean-usnob" width="160" height="160">
      </td>
      <td width="75%" valign="middle">
        <a href="https://huggingface.co/spaces/competitions/ship-detection">
          <span class="papertitle">Detecting Ships in Ports to Avoid Congestion and Manage Traffic</span>
        </a>
        <br>
        <em>HuggingFace Competition</em>, 2023
        <p>Exhausted Yolov5, v7 and v8 and push the mAP to 0.59. Learned at the end of the competition that all I needed to do is train and eval on evenly splitted images, so that object detection model can look at the images in a zoomed-in fashion.</p>
      </td>
    </tr>
  </tbody></table>

  <table width="100%" align="center" border="0" cellpadding="20"><tbody>
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/redditGPT.jpg" alt="clean-usnob" width="160" height="160">
      </td>
      <td width="75%" valign="middle">
        <a href="https://huggingface.co/spaces/mnauf/redditGPT">
          <span class="papertitle">RedditGPT</span>
        </a>
        <br>
        <p>Finetuned GPT2 on the recent public anonymous conversations from Reddit to capture the genuine public sentiment regarding the recent unfolding events in Pakistan since last year. Hosted project link here. Since this runs on CPU on huggingface free hosting, it takes about 500 seconds to respond. Alternatively, you can also run it on your GPU-enabled PC.</p>
      </td>
    </tr>
  </tbody></table>

  <table width="100%" align="center" border="0" cellpadding="20"><tbody>
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/gpt2.png" alt="clean-usnob" width="160" height="160">
      </td>
      <td width="75%" valign="middle">
        <a href="https://github.com/mnauf/GPT-from-scratch">
          <span class="papertitle">GPT2 from Scratch</span>
        </a>
        <br>
        <p>Add fundamental building blocks to the transformer decoder one by one and observe the resulting impact with each training. Trained 2 models. One of them on a Shakespeare dataset, and other on a small subset of OpenWebText.</p>
      </td>
    </tr>
  </tbody></table>

  </body>
</html>
