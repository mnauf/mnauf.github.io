<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Jon Barron</title>

    <meta name="author" content="Muhammad Naufil">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Muhammad Naufil
                </p>
                <p>I am a computer vision research engineer at <a href="https://retrocausal.ai">Retrocausal</a> in Karachi, where I work on human action recognition and 3D human pose estimation.
                </p>
                <p>
                  At Retrocausal I've worked on <a href="https://retrocausal.ai/">AI Copilot for Manufacturing Assembly Optimization</a>,  <a href="https://retrocausal.ai/guidance-analytics-and-trace/">Guidance Analytics and Trace</a> and <a href="https://retrocausal.ai/ergonomic-safety/">Ergonomic Safety</a>.
                </p>
                <p style="text-align:center">
                  <a href="mailto:m.naufil1@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="data/JonBarron-bio.txt">Bio</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=LHvJ9CUAAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/mnauf/">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/Muhammad_Naufil.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/photo_circle.png" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  I'm interested in Vision Language Models, 3D vision, 3D Human Pose Estimation, and Human Action Recognition.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          
    
      <tr onmouseout="zipnerf_stop()" onmouseover="zipnerf_start()"  bgcolor="#ffffd0">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='zipnerf_image'><video  width=100% height=100% muted autoplay loop>
            <source src="images/zipnerf.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
            <img src='images/zipnerf.jpg' width="160">
          </div>
          <script type="text/javascript">
            function zipnerf_start() {
              document.getElementById('zipnerf_image').style.opacity = "1";
            }

            function zipnerf_stop() {
              document.getElementById('zipnerf_image').style.opacity = "0";
            }
            zipnerf_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="http://jonbarron.info/zipnerf">
            <span class="papertitle">Permutation-Aware Action Segmentation via Unsupervised Frame-to-Segment Alignment</span>
          </a>
          <br>
          <a href="https://scholar.google.com.au/citations?user=8JbK0V8AAAAJ&hl=en">Quoc-Huy Tran</a>,
          <a href="https://pk.linkedin.com/in/ahmedrazamehmood">Ahmed Raza Mehmood</a>,
          <a href="https://pk.linkedin.com/in/muhammad4hmed">Muhammad Ahmed</a>,
          <strong>Muhammad Naufil</strong>,
          <a href="https://pk.linkedin.com/in/anas-zafar/">Anas Zafar</a>,
          <a href="https://www.linkedin.com/in/andreykonin">Andrey Konin</a>,
          <a href="https://scholar.google.com/citations?user=xE_pSDAAAAAJ&hl=en">Zeeshan Zia</a>
          <br>
          Submitted to <em>WACV</em>, 2023 &nbsp
          <br>
          <!-- <a href="http://jonbarron.info/zipnerf">project page</a> -->
          <!-- / -->
          <!-- <a href="https://www.youtube.com/watch?v=xrrhynRzC8k">video</a> -->
          <!-- / -->
          <a href="https://arxiv.org/pdf/2305.19478">arXiv</a>
          <p></p>
          <p>
            This paper presents an unsupervised transformer-based framework for temporal activity segmentation which leverages not only frame-level cues but also segment-level cues. This is in contrast with previous methods which often rely on frame-level information only. Our approach begins with a frame-level prediction module which estimates framewise action classes via a transformer encoder. 
            The frame-level prediction module is trained in an unsupervised manner via
            temporal optimal transport. To exploit segment-level information, we utilize a segment-level prediction module and a
            frame-to-segment alignment module. The former includes
            a transformer decoder for estimating video transcripts,
            while the latter matches frame-level features with segmentlevel features, yielding permutation-aware segmentation results. Moreover, inspired by temporal optimal transport,
            we introduce simple-yet-effective pseudo labels for unsupervised training of the above modules. Our experiments on
            four public datasets, i.e., 50 Salads, YouTube Instructions,
            Breakfast, and Desktop Assembly show that our approach
            achieves comparable or better performance than previous
            methods in unsupervised activity segmentation.          
          </p>
        </td>
      </tr>

      <tr onmouseout="zipnerf_stop()" onmouseover="zipnerf_start()"  bgcolor="#ffffd0">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='zipnerf_image'><video  width=100% height=100% muted autoplay loop>
            <source src="images/zipnerf.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
            <img src='images/zipnerf.jpg' width="160">
          </div>
          <script type="text/javascript">
            function zipnerf_start() {
              document.getElementById('zipnerf_image').style.opacity = "1";
            }

            function zipnerf_stop() {
              document.getElementById('zipnerf_image').style.opacity = "0";
            }
            zipnerf_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="http://jonbarron.info/zipnerf">
            <span class="papertitle">PeAction Segmentation Using 2D Skeleton Heatmaps</span>
          </a>
          <br>
          <a href="https://pk.linkedin.com/in/syedwaleedhyder">Syed Waleed Haider</a>,
          <a href="https://pk.linkedin.com/in/muhammad-usama-630722206">Muhammad Usama</a>,
          <a href="https://pk.linkedin.com/in/anas-zafar/">Anas Zafar</a>,
          <strong>Muhammad Naufil</strong>,
          <a href="https://www.linkedin.com/in/andreykonin">Andrey Konin</a>,
          <a href="https://scholar.google.com/citations?user=xE_pSDAAAAAJ&hl=en">Zeeshan Zia</a>
          <a href="https://scholar.google.com.au/citations?user=8JbK0V8AAAAJ&hl=en">Quoc-Huy Tran</a>,
          <br>
          Submitted to <em>ICRA</em>, 2023 &nbsp
          <br>
          <!-- <a href="http://jonbarron.info/zipnerf">project page</a> -->
          <!-- / -->
          <!-- <a href="https://www.youtube.com/watch?v=xrrhynRzC8k">video</a> -->
          <!-- / -->
          <a href="https://arxiv.org/pdf/2309.06462.pdf">arXiv</a>
          <p></p>
          <p>
            This paper presents a 2D skeleton-based action
            segmentation method with applications in fine-grained human
            activity recognition. In contrast with state-of-the-art methods
            which directly take sequences of 3D skeleton coordinates
            as inputs and apply Graph Convolutional Networks (GCNs)
            for spatiotemporal feature learning, our main idea is to use
            sequences of 2D skeleton heatmaps as inputs and employ
            Temporal Convolutional Networks (TCNs) to extract spatiotemporal features. Despite lacking 3D information, our approach
            yields comparable/superior performances and better robustness
            against missing keypoints than previous methods on action
            segmentation datasets. Moreover, we improve the performances
            further by using both 2D skeleton heatmaps and RGB videos
            as inputs. To our best knowledge, this is the first work to utilize
            2D skeleton heatmap inputs and the first work to explore 2D
            skeleton+RGB fusion for action segmentation.        
          </p>
        </td>
      </tr>


            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/clean_promo.jpg" alt="clean-usnob" width="160" height="160">
              </td>
              <td width="75%" valign="middle">
                <a href="https://drive.google.com/file/d/1YvRx-4hrZoCk-nl6OgVJZlHAqOiN5hWq/view?usp=sharing">
                  <span class="papertitle">Cleaning the USNO-B Catalog Through Automatic Detection of Optical Artifacts</span>
                </a>
                <br>
                <strong>Jonathan T. Barron</strong>, <a href="http://stumm.ca/">Christopher Stumm</a>, <a href="http://cosmo.nyu.edu/hogg/">David W. Hogg</a>, <a href="http://www.astro.princeton.edu/~dstn/">Dustin Lang</a>, <a href="http://cs.nyu.edu/~roweis/">Sam Roweis</a>
                <br>
                <em>The Astronomical Journal</em>, 135, 2008
                <p>We use computer vision techniques to identify and remove diffraction spikes and reflection halos in the USNO-B Catalog.</p>
                <p>In use at <a href="http://www.astrometry.net">Astrometry.net</a></p>
              </td>
            </tr>

          </tbody></table>

          
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Miscellanea</h2>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cvf.jpg"></td>
              <td width="75%" valign="center">
                <a href="https://cvpr2023.thecvf.com/Conferences/2023/Organizers">Demo Chair, CVPR 2023</a>
          <br>
                <a href="https://cvpr2022.thecvf.com/area-chairs">Area Chair, CVPR 2022</a>
                <br>
                <a href="http://cvpr2021.thecvf.com/area-chairs">Area Chair & Longuet-Higgins Award Committee Member, CVPR 2021</a>
                <br>
                <a href="http://cvpr2019.thecvf.com/area_chairs">Area Chair, CVPR 2019</a>
                <br>
                <a href="http://cvpr2018.thecvf.com/organizers/area_chairs">Area Chair, CVPR 2018</a>
              </td>
            </tr>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/cs188.jpg" alt="cs188">
              </td>
              <td width="75%" valign="center">
                <a href="http://inst.eecs.berkeley.edu/~cs188/sp11/announcements.html">Graduate Student Instructor, CS188 Spring 2011</a>
                <br>
                <a href="http://inst.eecs.berkeley.edu/~cs188/fa10/announcements.html">Graduate Student Instructor, CS188 Fall 2010</a>
                <br>
                <a href="http://aima.cs.berkeley.edu/">Figures, "Artificial Intelligence: A Modern Approach", 3rd Edition</a>
              </td>
            </tr>
            

            <tr>
              <td align="center" style="padding:20px;width:25%;vertical-align:middle">
                <h2>Basically <br> Blog Posts</h2>
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/abs/2112.11687">Squareplus: A Softplus-Like Algebraic Rectifier</a>
                <br>
                <a href="https://arxiv.org/abs/2010.09714">A Convenient Generalization of Schlick's Bias and Gain Functions</a>
                <br>
                <a href="https://arxiv.org/abs/1704.07483">Continuously Differentiable Exponential Linear Units</a>
                <br>
                <a href="https://jonbarron.info/data/cvpr2023_llm_workshop_annotated.pdf">Scholars & Big Models: How Can Academics Adapt?</a>
              </td>
            </tr>
            
            
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Feel free to steal this website's <a href="https://github.com/jonbarron/jonbarron_website">source code</a>. <strong>Do not</strong> scrape the HTML from this page itself, as it includes analytics tags that you do not want on your own website &mdash; use the github code instead. Also, consider using <a href="https://leonidk.com/">Leonid Keselman</a>'s <a href="https://github.com/leonidk/new_website">Jekyll fork</a> of this page.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
