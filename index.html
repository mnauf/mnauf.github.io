<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Jon Barron</title>

    <meta name="author" content="Muhammad Naufil">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Muhammad Naufil
                </p>
                <p>I am a computer vision research engineer at <a href="https://retrocausal.ai">Retrocausal</a> in Karachi, where I work on human action recognition and 3D human pose estimation.
                </p>
                <p>
                  At Retrocausal I've worked on <a href="https://retrocausal.ai/">AI Copilot for Manufacturing Assembly Optimization</a>,  <a href="https://retrocausal.ai/guidance-analytics-and-trace/">Guidance Analytics and Trace</a> and <a href="https://retrocausal.ai/ergonomic-safety/">Ergonomic Safety</a>.
                </p>
                <p style="text-align:center">
                  <a href="mailto:m.naufil1@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=LHvJ9CUAAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/mnauf/">Github</a>
                  <a href="https://www.linkedin.com/in/mnauf/">Linkedin</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/Muhammad_Naufil.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/photo_circle.png" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  I'm interested in Vision Language Models, 3D vision, 3D Human Pose Estimation, and Human Action Recognition.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          
    
      <tr onmouseout="zipnerf_stop()" onmouseover="zipnerf_start()"  bgcolor="#FFFFFF">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='zipnerf_image'><video  width=100% height=100% muted autoplay loop>
            <source src="images/zipnerf.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
            <img src='images/zipnerf.jpg' width="160">
          </div>
          <script type="text/javascript">
            function zipnerf_start() {
              document.getElementById('zipnerf_image').style.opacity = "1";
            }

            function zipnerf_stop() {
              document.getElementById('zipnerf_image').style.opacity = "0";
            }
            zipnerf_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="http://jonbarron.info/zipnerf">
            <span class="papertitle">Permutation-Aware Action Segmentation via Unsupervised Frame-to-Segment Alignment</span>
          </a>
          <br>
          <a href="https://scholar.google.com.au/citations?user=8JbK0V8AAAAJ&hl=en">Quoc-Huy Tran</a>,
          <a href="https://pk.linkedin.com/in/ahmedrazamehmood">Ahmed Raza Mehmood</a>,
          <a href="https://pk.linkedin.com/in/muhammad4hmed">Muhammad Ahmed</a>,
          <strong>Muhammad Naufil</strong>,
          <a href="https://pk.linkedin.com/in/anas-zafar/">Anas Zafar</a>,
          <a href="https://www.linkedin.com/in/andreykonin">Andrey Konin</a>,
          <a href="https://scholar.google.com/citations?user=xE_pSDAAAAAJ&hl=en">Zeeshan Zia</a>
          <br>
          Submitted to <em>WACV</em>, 2023 &nbsp
          <br>
          <!-- <a href="http://jonbarron.info/zipnerf">project page</a> -->
          <!-- / -->
          <!-- <a href="https://www.youtube.com/watch?v=xrrhynRzC8k">video</a> -->
          <!-- / -->
          <a href="https://arxiv.org/pdf/2305.19478">arXiv</a>
          <p></p>
          <p>
            This paper presents an unsupervised transformer-based framework for temporal activity segmentation which leverages not only frame-level cues but also segment-level cues. This is in contrast with previous methods which often rely on frame-level information only. Our approach begins with a frame-level prediction module which estimates framewise action classes via a transformer encoder. 
            The frame-level prediction module is trained in an unsupervised manner via
            temporal optimal transport. To exploit segment-level information, we utilize a segment-level prediction module and a
            frame-to-segment alignment module. The former includes
            a transformer decoder for estimating video transcripts,
            while the latter matches frame-level features with segmentlevel features, yielding permutation-aware segmentation results. Moreover, inspired by temporal optimal transport,
            we introduce simple-yet-effective pseudo labels for unsupervised training of the above modules. Our experiments on
            four public datasets, i.e., 50 Salads, YouTube Instructions,
            Breakfast, and Desktop Assembly show that our approach
            achieves comparable or better performance than previous
            methods in unsupervised activity segmentation.          
          </p>
        </td>
      </tr>

      <tr onmouseout="zipnerf_stop()" onmouseover="zipnerf_start()"  bgcolor="#FFFFFF">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='zipnerf_image'><video  width=100% height=100% muted autoplay loop>
            <source src="images/zipnerf.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
            <img src='images/zipnerf.jpg' width="160">
          </div>
          <script type="text/javascript">
            function zipnerf_start() {
              document.getElementById('zipnerf_image').style.opacity = "1";
            }

            function zipnerf_stop() {
              document.getElementById('zipnerf_image').style.opacity = "0";
            }
            zipnerf_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="http://jonbarron.info/zipnerf">
            <span class="papertitle">Action Segmentation Using 2D Skeleton Heatmaps</span>
          </a>
          <br>
          <a href="https://pk.linkedin.com/in/syedwaleedhyder">Syed Waleed Haider</a>,
          <a href="https://pk.linkedin.com/in/muhammad-usama-630722206">Muhammad Usama</a>,
          <a href="https://pk.linkedin.com/in/anas-zafar/">Anas Zafar</a>,
          <strong>Muhammad Naufil</strong>,
          <a href="https://www.linkedin.com/in/andreykonin">Andrey Konin</a>,
          <a href="https://scholar.google.com/citations?user=xE_pSDAAAAAJ&hl=en">Zeeshan Zia</a>,
          <a href="https://scholar.google.com.au/citations?user=8JbK0V8AAAAJ&hl=en">Quoc-Huy Tran</a>,
          <br>
          Submitted to <em>ICRA</em>, 2023 &nbsp
          <br>
          <!-- <a href="http://jonbarron.info/zipnerf">project page</a> -->
          <!-- / -->
          <!-- <a href="https://www.youtube.com/watch?v=xrrhynRzC8k">video</a> -->
          <!-- / -->
          <a href="https://arxiv.org/pdf/2309.06462.pdf">arXiv</a>
          <p></p>
          <p>
            This paper presents a 2D skeleton-based action
            segmentation method with applications in fine-grained human
            activity recognition. In contrast with state-of-the-art methods
            which directly take sequences of 3D skeleton coordinates
            as inputs and apply Graph Convolutional Networks (GCNs)
            for spatiotemporal feature learning, our main idea is to use
            sequences of 2D skeleton heatmaps as inputs and employ
            Temporal Convolutional Networks (TCNs) to extract spatiotemporal features. Despite lacking 3D information, our approach
            yields comparable/superior performances and better robustness
            against missing keypoints than previous methods on action
            segmentation datasets. Moreover, we improve the performances
            further by using both 2D skeleton heatmaps and RGB videos
            as inputs. To our best knowledge, this is the first work to utilize
            2D skeleton heatmap inputs and the first work to explore 2D
            skeleton+RGB fusion for action segmentation.        
          </p>
        </td>
      </tr>


      <tr onmouseout="zipnerf_stop()" onmouseover="zipnerf_start()"  bgcolor="#FFFFFF">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='zipnerf_image'><video  width=100% height=100% muted autoplay loop>
            <source src="images/zipnerf.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
            <img src='images/zipnerf.jpg' width="160">
          </div>
          <script type="text/javascript">
            function zipnerf_start() {
              document.getElementById('zipnerf_image').style.opacity = "1";
            }

            function zipnerf_stop() {
              document.getElementById('zipnerf_image').style.opacity = "0";
            }
            zipnerf_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="http://jonbarron.info/zipnerf">
            <span class="papertitle">AI-mediated Job Status Tracking in AR as a No-Code service</span>
          </a>
          <br>
          <a href="https://www.linkedin.com/in/andreykonin">Andrey Konin</a>,
          <a href="https://pk.linkedin.com/in/shakeebsiddiqui">Shakeeb Siddiqui</a>,
          <a href="https://pk.linkedin.com/in/hasan-gilani">Hasan Gilani</a>,
          <a href="https://pk.linkedin.com/in/muhammad-mudassir-7024391ab">Muhammad Mudassir</a>,
          <a href="https://pk.linkedin.com/in/hassan11196">Hassan Ahmed</a>,
          <a href="https://www.linkedin.com/in/tabanshaukat/">Taban Shaukat</a>,
          <strong>Muhammad Naufil</strong>,
          <a href="https://scholar.google.com.pk/citations?user=NaF7zqYAAAAJ&hl=en">Awais Ahmed</a>,
          <a href="https://scholar.google.com.au/citations?user=8JbK0V8AAAAJ&hl=en">Quoc-Huy Tran</a>,
          <a href="https://scholar.google.com/citations?user=xE_pSDAAAAAJ&hl=en">Zeeshan Zia</a>
          <br>
          Published in <em>ISMAR</em>, 2022 &nbsp
          <br>
          <a href="https://ieeexplore.ieee.org/document/9974204">IEEE</a>
          <p></p>
          <p>
            Sophisticated AR experiences that can track the status of a manual assembly or maintenance job and provide corresponding guidance are built by teams of computer vision engineers over several months. These engineers model temporal causation on top of object detection and human skeletal tracking provided by traditional AR and AI SDKs. We demonstrate our Pathfinder system which automatically builds computational models of a complex manual task, such as a manufacturing assembly activity, given recorded demonstrations of the task - without requiring any custom coding. These computational models enable ordinary AR developers to build AI-mediated feedback in minutes instead of months. Keywords: Task Guidance, Computer Vision, Human Computer Interaction, Machine Learning, Industrial Applications     
          </p>
        </td>
      </tr>


      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="https://user-images.githubusercontent.com/41832069/246027006-5fe0e6e5-1964-49b9-843e-9fcead65da05.png" alt="clean-usnob" width="160" height="160">
        </td>
        <td width="75%" valign="middle">
          <a href="https://github.com/mnauf/self-driving-car-fyp/blob/main/FYP%20REPORT.pdf">
            <span class="papertitle">SUPERVISED LEARNING BASED AUTONOMOUS CAR</span>
          </a>
          <br>
          <strong>Muhammad Naufil</strong>, <a href="https://www.linkedin.com/in/faiz-ur-rehman/">Faiz ur Rehman</a>, <a href="https://pk.linkedin.com/in/syed-haziq-222b94193">Syed Muhammad Haziq</a>, <a href="https://www.linkedin.com/in/danish-khalid-aa8293137/">Danish Khalid</a>
          <br>
          <em>Bachelor Thesis Project</em>, 2020 at NED University of Engineering & Technology
          <p>This report explains the challenges, design details of supervised learning autonomous cars and also covers the costs and schedule. The goal of the project is to make the prototype for an autonomous car in low cost, low power which can drive itself on custom made road without the interference of humans and detect the object as well avoid it with the help of the camera.</p>
        </td>
      </tr>

  </body>
</html>
